{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hdDofDXYFTrruxfHz7PPUUMyK4qQxhmZ","timestamp":1667500064631}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<font color=\"#de3023\"><h1><b>REMINDER: MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>\n","\n","# Lab 10: RNNs\n","Now that we've finished working through CNNs and computer vision, we're going to move onto the last major topic of the course: RNNs and NLP. RNN stands for \"Recurrent Neural Network\" and NLP for \"Natural Language Processing.\" The field of NLP has existed for some time before RNNs and modern machine learning, but it has really taken large strides recently because of RNNs. We'll cover NLP in some detail next week. Today, we'll just be talking about the basics of RNNs."],"metadata":{"id":"Bdhj_95gFUEj"}},{"cell_type":"markdown","source":["## Why RNNs?\n","We've already discussed a ton of deep learning tools over this course:\n","- Dense layers\n","- Dropout\n","- Convolutions\n","- MaxPooling\n","\n","So why add another? It's useful to see where the core deficiency lies with what we've learned so far. One of the main ones is that in *all* the data we've worked with so far, we've treated each prediction as being totally independent of one another. In all the datasets we've worked with so far, this assumption makes complete sense: after all, if you predict one image to be a dog, it makes no difference for how you will predict the next image you see. This is totally untrue, however, in some other datasets. Suppose we had a video where we wish to classify if there's a dog in the frame or not for each frame. If there's a dog that's there in the current frame, chances are it will be there in the next frame as well! So, in this case, we *could* continue to treat the predictions as independent and ignroe the history of things we had seen up to that point, but it seems likely that doing so would leave some accuracy on the table for us to scoop up.\n","\n","Datasets that have this time-dependent property often fall into the field of \"time series\" analysis, although there are other cases where RNNs are useful, such as in NLP. In language, properties of the current word in a sentence are greatly influenced by those that preceded (or those that succeed) it, which is why we want to take these things into account. RNNs are what let us do this: they give us the ability to \"keep track\" of what we've seen so far in making predictions."],"metadata":{"id":"vfCPVxgBGOH7"}},{"cell_type":"markdown","source":["## Use Cases\n","RNNs can be used for a *number* of cases:\n","\n","![](https://cs231n.github.io/assets/rnn/types.png)\n","\n","- One-to-one: what we've been doing so far\n","- One-to-many: Text generation\n","- Many-to-one: Sentiment analysis\n","- Many-to-many: Stock predictions\n","\n","The main unifying theme, and when you should have sirens in your head to signal the use of RNNs, is when your data is structured in a way where there is dependence, specifically where the past (or future, but we'll ignore this for now) is useful in predicting the next step. \n","\n","An important note is that this dependence **need not** be in your predictions! Sometimes it will be, which is why this could be confusing, but in general, the use of RNNs is a function of your **data (X)**. This is similar to how we exploited the structure of images in choosing to incorporate convolutions into our networks: it wasn't the **output** types that affected this choice of architecture. It was the data. So, in choosing your architectures, you should focus on properties of the data, **not** the labels. Keep \"sentiment analysis\" as a good example for this, since the data are sequential, but the final prediction is just a single overall label."],"metadata":{"id":"AwFFVwO6TOd3"}},{"cell_type":"markdown","source":["## Overall Architecture\n","Great! Now that we've got the main conceptual motivation out, let's see how these things actually work in practice. These architectures can be pretty involved, so we'll dive into specifics later and just walk through the high-level ideas first. The idea is that the network is going to learn how to \"store\" a representation of the things that it has seen in the past into a hidden state. This, **combined with** the next data point, is what's used to predict the next y!\n","\n","![](https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232)\n","\n","There are a bunch of different RNNs, but we've gone through a fair bit of theory at this point. So, let's switch gears to see how this is used in practice."],"metadata":{"id":"toyDZkd_TlaF"}},{"cell_type":"markdown","source":["## Setup\n","For today's lab, we'll start by looking at how to use RNNs for stock prediction:"],"metadata":{"id":"eaztOuyXwwCT"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import pandas as pd\n","import numpy as np\n","import urllib.request\n","import matplotlib.pyplot as plt\n","\n","url = \"https://raw.githubusercontent.com/kevincwu0/rnn-google-stock-prediction/master/Google_Stock_Price_Train.csv\"\n","urllib.request.urlretrieve(url, \"Google_Stock_Price_Train.csv\")\n","\n","url = \"https://raw.githubusercontent.com/kevincwu0/rnn-google-stock-prediction/master/Google_Stock_Price_Test.csv\"\n","urllib.request.urlretrieve(url, \"Google_Stock_Price_Test.csv\")"],"metadata":{"id":"8ZLjjUotwE5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One thing to keep in mind is how we construct the datasets: now that we're actually baking in information about the past for predicting the future, we have to construct the dataset accordingly to account for this! Let's start by loading in the raw datset:"],"metadata":{"id":"kkeIHw7Mdfic"}},{"cell_type":"code","source":["dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n","training_set = dataset_train.iloc[:, 1:2].values \n","\n","from sklearn.preprocessing import MinMaxScaler\n","sc = MinMaxScaler(feature_range = (0, 1))\n","training_set_scaled = sc.fit_transform(training_set)"],"metadata":{"id":"gg9uPFgpXJ56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(training_set_scaled[0:10])"],"metadata":{"id":"shMVBPYMep9O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(training_set, color = 'red', label = 'Real Google Stock Price')\n","plt.title('Google Stock Price Prediction')\n","plt.xlabel('Time')\n","plt.ylabel('Google Stock Price')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"5PYc0R_pexn3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unlike past work you've done, this data can't be used as is for training: you have to construct the serieses yourself that will be used. Let's give that a go:"],"metadata":{"id":"8hAi5kBZd9yL"}},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# Construct datasets X_train and y_train, which are, for each time step, the#\n","# price in the last 60 time steps and the current price resp.               #\n","# The dataset you should pull from is: training_set_scaled                  #\n","#############################################################################\n","\n","num_pts, _ = training_set_scaled.shape\n","X_train = []\n","y_train = []\n","\n","for i in range(____):\n","  ____\n","  ____\n","\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","print(y_train.shape)\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"6tH3HYg2d-0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SimpleRNNs\n","Let's start by seeing how to use RNNs for this task. We will use a new layer called `SimpleRNN` to get started:"],"metadata":{"id":"Ht0PRRJde6pM"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import SimpleRNN\n","from keras.layers import Dropout"],"metadata":{"id":"KWXQzpJdZ-Vo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`SimpleRNN` is a basic RNN: we'll see extensions soon. The main two parameters to keep in mind are:\n","\n","- `units`: The dimension of the hidden state: this does **not** have to match the window size! It's just another hyperparameter to choose\n","- `return_sequences`: Whether you want to return the **entire** prediction on the input data sequence or **just** the prediction on the final point of the sequence \n","\n","Let's see how the results differ before we construct a full RNN:"],"metadata":{"id":"_gMEa-_SfLK_"}},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# Construct a SimpleRNN with 50 units (and return_sequences False) and pass \n","# in an example from your training set through the layer. What do you expect \n","# the dimension to be?\n","#############################################################################\n","\n","simple_rnn = ____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"2n1s5bnyf9vs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model(X_train[0:1,:])"],"metadata":{"id":"Sz8RjlwuWJUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# Repeat the above, now with return_sequences = True. What do you expect \n","# the dimension to be now? \n","#############################################################################\n","\n","____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"6Oq4YdcTggIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great! So, `return_sequences` is mostly useful when you either want to stack multiple LSTMs together *or* if you want to do a many-to-many prediction (like part of speech tagging in a block of text for instance). Let's now think about what we should be using for this task. The architecture of the model will be:\n","\n","```\n","SimpleRNN -> Dropout -> Dense\n","```\n","\n","Think about what the task is and what the output dimension should be as a result!"],"metadata":{"id":"j7xUXJtChR53"}},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                              #\n","#                                                                           #\n","# Construct a sequential model with the architecture given above. Don't \n","# train it yet: we'll do that next\n","#############################################################################\n","\n","model = tf.keras.models.Sequential(\n","  ____\n",")\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"YjWcoxqtZ_Or"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# Compile your model with the appropriate loss. Remember, we're doing a \n","# regression task here!\n","#############################################################################\n","\n","model.compile(____)\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################\n","\n","model.fit(X_train, y_train, epochs = 20, batch_size = 32)"],"metadata":{"id":"cJDsXKGviwD4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now take a look at the results:"],"metadata":{"id":"IQUzTNcAjCX_"}},{"cell_type":"code","source":["dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n","real_stock_price = dataset_test.iloc[:, 1:2].values"],"metadata":{"id":"LKDUrTjXaRPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_model(model):\n","  dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n","  real_stock_price = dataset_test.iloc[:, 1:2].values\n","\n","  # Getting the predicted stock price of 2017\n","  dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n","  inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n","  inputs = inputs.reshape(-1,1)\n","  inputs = sc.transform(inputs)\n","  X_test = []\n","\n","  for i in range(60, 80):\n","      X_test.append(inputs[i-60:i, 0])\n","\n","  X_test = np.array(X_test)\n","  X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n","  predicted_stock_price = model.predict(X_test)\n","  predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n","\n","  # Visualising the results\n","  plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n","  plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n","  plt.title('Google Stock Price Prediction')\n","  plt.xlabel('Time')\n","  plt.ylabel('Google Stock Price')\n","  plt.legend()\n","  plt.show()"],"metadata":{"id":"JAFbBlZraUOJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_model(model)"],"metadata":{"id":"utcvkkGKo9uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTMs"],"metadata":{"id":"BexVBrtidA-L"}},{"cell_type":"markdown","source":["One of the main issues with this SimpleRNN is that, due to how the hidden state is learned, it is unable to keep track of long-term dependencies. This means, if you consider longer and longer sequences, the values at the beginning of the sequence will not have any impact on the predictions made later in the sequence, which totally defeats the purpose of using RNNs in the first place. For this reason, we need to figure out a way of learning these longer form dependencies. Luckily for us, there has been a great deal of work that was put into fixing exactly this problem. For this reason, people have come up with something called a \"Long Short-Term Memory\" cell. Before we actually understand how they work, let's see how simple it is to use them: just replace the SimpleRNN with this LSTM!"],"metadata":{"id":"DcOwBem9jX1x"}},{"cell_type":"code","source":["from keras.layers import LSTM"],"metadata":{"id":"b4vqilmVc8bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# Construct a sequential model now with an LSTM and retrain it\n","#############################################################################\n","\n","model = ____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################\n","\n","plot_model(model)"],"metadata":{"id":"qDzhIc0BctVQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Opening the Black Box\n","Let's now take a look inside the SimpleRNN model. Remember that a RNN can be represented by a sequence of hidden states $h_1, ..., h_t$, where each hidden state $h_t$ can be computed as some function of the previous hidden state $h_{t-1}$, the current input $x_t$, a set of weights $W$:\n","\n","$$h_t = f_W(h_{t-1}, x_t)$$\n","\n","The point of an RNN is to learn these parameters weights $W$ to make it possible to construct the hidden state on the fly. So, what is this function we're actually using? It's:\n","\n","$$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n","\n","We typically ignore the bias, so we'll drop that for the time being. Let's take a moment to parse this. Visually, this is the same as:\n","\n","![](https://cs231n.github.io/assets/rnn/vanilla_rnn_mformula_1.png)\n","\n","Conceptually, this looks pretty similar to stuff we've done in the past! Remember that a fully-connected layer is nothing but:\n","\n","$$y_t = tanh(W_{xy}x_t)$$\n","\n","(Where here, we're just using a tanh activation instead of a ReLU) So, we're doing nothing but iterating through the input, with the hidden state saved and used for the next prediction, where we now have **two** weight matrices: one for the hidden state and the other for the input!"],"metadata":{"id":"9K49cDUCdCJM"}},{"cell_type":"code","source":["hidden_dim = 50\n","x_dim = X_train[0, :].shape[0]"],"metadata":{"id":"FJCH_WoTzgNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h = tf.Variable(tf.zeros((1, hidden_dim)))\n","x = tf.Variable(tf.zeros((1, x_dim)))"],"metadata":{"id":"PBTwr8ww1VSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# Construct the two weight matrices (call them \"Whh\" and \"Wxh\") that are the\n","# correct dimensions from the equation above. Just fill them with 0s. Remember:\n","# h_t = tanh(W_{hh} h_{t-1} + W_{xh}x_t)\n","#############################################################################\n","\n","Whh = ____\n","Wxh = ____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"fiEnhO_u1p7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# Using the weights matrices you defined in the cell above, make predictions\n","# for the \"current\" hidden state h and input x\n","#############################################################################\n","print(x.shape)\n","h = ____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"Zwq7Fny7c3oF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h.shape"],"metadata":{"id":"FtNpSSONb48r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp.shape"],"metadata":{"id":"NAvEyVuFb0zU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now try to see how we would use this basic framework to actually model a particular image or sentence:"],"metadata":{"id":"rpLXVA_Z5Avd"}},{"cell_type":"code","source":["x = tf.Variable(tf.zeros((5, x_dim))) # make up data of sequence size 5\n","h = tf.Variable(tf.zeros((1, hidden_dim)))"],"metadata":{"id":"aONqyXdF5OQA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                               #\n","#                                                                           #\n","# String the code from above together to make predictions on the *entire* \n","# sequence x now. Remember: you update the hidden state and use that with\n","# each successive x to make a prediction!\n","#############################################################################\n","\n","h = tf.Variable(tf.zeros((1, hidden_dim)))\n","num_examples, _  =  x.shape\n","\n","for i in range(num_examples):\n","  ____\n","print(h)\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"vv85pTDN3ysV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                              #\n","#                                                                           #\n","# Let's try putting it all together into a single function: this function\n","# receives an input and you have to return the *final* state h. Use zeros\n","# for your initial hidden state. Make sure to also construct the weights\n","# with the appropriate sizes\n","#############################################################################\n","\n","def simple_rnn(x, hidden_dim):\n","  num_examples, x_dim  =  x.shape\n","  Whh = ____\n","  Wxh = ____\n","\n","  h = ____\n","\n","  for i in range(num_examples):\n","    ____\n","  return h\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################\n","test_x = tf.Variable(tf.zeros((20, 5))) \n","print(simple_rnn(test_x, 10).shape)"],"metadata":{"id":"153TzGig57cs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And that's basically it for SimpleRNNs!"],"metadata":{"id":"TsObQuV3794m"}},{"cell_type":"markdown","source":["## LSTMs in Depth\n","Finally, let's take a look at LSTMs. The main difference, as we mentioned, is **how** the hidden state is tracked:\n","\n","![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n","\n","There are a *lot* of moving parts here, but the core idea is that we're holding onto another state (called the \"cell state\") in addition to the hidden state. The hand-wavy intuition is that, by only tracking relevant things in the hidden state instead of the overwriting that happens at each step of a vanilla RNN, we are able to avoid \"polluting\" the memory of the RNN and therefore improve the long-term memory of the unit. The top line in the diagram above represents this cell state, and you can see how information from h and x are regulated by these \"sigmoid gates\" to actually impacting the cell state.\n","\n","Remember: sigmoid outputs values between [0,1], so this roughly corresponds to \"how much\" of the input should be let through to the cell state. The rough ideas for the gates are as follows:\n","\n","- Forget gate: How much of the cell state to just throw away\n","- Input gate: How should the cell state be updated to incorporate the new data that's coming in from x and h\n","- Output gate: How should the cell state by used in combinating with the hidden and x to produce another output\n","\n","Thinking of the cell state as long-term memory and the hidden state as short term memory is a very crude intuition you can keep in mind. In math, we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n","\n","$$\n","\\begin{align*}\n","i = \\sigma(a_i) \\hspace{2pc}\n","f = \\sigma(a_f) \\hspace{2pc}\n","o = \\sigma(a_o) \\hspace{2pc}\n","g = \\tanh(a_g)\n","\\end{align*}\n","$$\n","\n","We compute the next cell state $c_t$ and next hidden state $h_t$ as\n","\n","$$\n","c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n","h_t = o\\odot\\tanh(c_t)\n","$$\n","\n","Let's see how to begin to implement this:"],"metadata":{"id":"2pOec9sG8POu"}},{"cell_type":"code","source":["cell_dim = 25\n","hidden_dim = 50\n","x_dim = X_train[0, :].shape[0]\n","\n","x = tf.Variable(tf.zeros((1, x_dim)))\n","c = tf.Variable(tf.zeros((1, hidden_dim)))"],"metadata":{"id":"PdULbba16vFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                              #\n","#                                                                           #\n","# Construct the weight matrices AND h vector now to fit the dimensions for \n","# the LSTM eqns\n","#############################################################################\n","\n","____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"cKMgB0pGAR2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                              #\n","#                                                                           #\n","# Compute the four gates for the given x, h above. Remember the g gate is\n","# different from the rest\n","#############################################################################\n","\n","____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"NUzNVTGiAn4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################################################################\n","# Exercise                                                             #\n","#                                                                           #\n","# Compute the update to the cell state and that to the hidden state\n","#############################################################################\n","\n","____\n","\n","#############################################################################\n","#                              END OF YOUR CODE                             #\n","#############################################################################"],"metadata":{"id":"hsqU9ckoB_Go"},"execution_count":null,"outputs":[]}]}