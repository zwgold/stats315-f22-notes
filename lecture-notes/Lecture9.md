# STATS 315: Lecture 9 (Shallow NN I)
* Note: Missed Lecture 8 (Vectorization and Linear Algebra Bootcamp II) due to Amazon interview, will need to use PDF notes for reference

## Goals
* Geometric view of LA
* Shallow NN
    * From logistic regression (one output NN) to NNs (many layers of output neurons)

## Wrapping up Matrices + Vectors
 
### Geometric View of Linear Algebra

#### Takeaways
* Vectors can be seen as points or directions in space
* Dot products define the notion of angle to arbitrarily high-dimensional space
* Hyperplanes are high-dimensional generalizations of lines and planes. They can be used to define decision planes that are often used as the last step in a classification task
* Matrix Multiplication can be geometrically interpreted as uniform distortions of the underlying coordinates
* Linear dependence is a way to tell when a collection of vectors are in a lower dimensional space than we expected
* When a matrix's inverse is defined, matrix inversion gives another matrix that undoes the action of the first. 
* Tensor contractions and Einstein summation provide for a neat and clean notation for expressing many of the computations that are seen in machine learning.

## New Stuff
* Literally all review tbh