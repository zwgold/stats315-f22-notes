{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cifup1gR3_QH"
      },
      "source": [
        "**Notebook credit**: Based on the original D2L notebook [here](https://github.com/d2l-ai/d2l-en-colab/blob/master/chapter_convolutional-neural-networks/conv-layer.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "eF7pcCKb3_QL"
      },
      "source": [
        "# Convolutions for Images\n",
        "\n",
        "\n",
        "Now that we understand how convolutional layers work in theory,\n",
        "we are ready to see how they work in practice.\n",
        "Building on our motivation of convolutional neural networks\n",
        "as efficient architectures for exploring structure in image data,\n",
        "we stick with images as our running example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The Cross-Correlation Operation\n",
        "\n",
        "Recall that strictly speaking, convolutional layers\n",
        "are a  misnomer, since the operations they express\n",
        "are more accurately described as cross-correlations.\n",
        "Based on our descriptions of convolutional layers,\n",
        "in such a layer, an input tensor\n",
        "and a kernel tensor are combined\n",
        "to produce an output tensor through a (**cross-correlation operation.**)\n",
        "\n",
        "Let us ignore channels for now and see how this works\n",
        "with two-dimensional data and hidden representations.\n",
        "In the figure below,\n",
        "the input is a two-dimensional tensor\n",
        "with a height of 3 and width of 3.\n",
        "We mark the shape of the tensor as $3 \\times 3$ or ($3$, $3$).\n",
        "The height and width of the kernel are both 2.\n",
        "The shape of the *kernel window* (or *convolution window*)\n",
        "is given by the height and width of the kernel\n",
        "(here it is $2 \\times 2$).\n",
        "\n",
        "![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+1\\times1+3\\times2+4\\times3=19$.](http://d2l.ai/_images/correlation.svg)\n",
        "\n",
        "\n",
        "In the two-dimensional cross-correlation operation,\n",
        "we begin with the convolution window positioned\n",
        "at the upper-left corner of the input tensor\n",
        "and slide it across the input tensor,\n",
        "both from left to right and top to bottom.\n",
        "When the convolution window slides to a certain position,\n",
        "the input subtensor contained in that window\n",
        "and the kernel tensor are multiplied elementwise\n",
        "and the resulting tensor is summed up\n",
        "yielding a single scalar value.\n",
        "This result gives the value of the output tensor\n",
        "at the corresponding location.\n",
        "Here, the output tensor has a height of 2 and width of 2\n",
        "and the four elements are derived from\n",
        "the two-dimensional cross-correlation operation:\n",
        "\n",
        "$$\n",
        "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
        "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
        "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
        "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
        "$$\n",
        "\n",
        "Note that along each axis, the output size\n",
        "is slightly smaller than the input size.\n",
        "Because the kernel has width and height greater than one,\n",
        "we can only properly compute the cross-correlation\n",
        "for locations where the kernel fits wholly within the image,\n",
        "the output size is given by the input size $n_h \\times n_w$\n",
        "minus the size of the convolution kernel $k_h \\times k_w$\n",
        "via\n",
        "\n",
        "$$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n",
        "\n",
        "This is the case since we need enough space\n",
        "to \"shift\" the convolution kernel across the image.\n",
        "Later we will see how to keep the size unchanged\n",
        "by padding the image with zeros around its boundary\n",
        "so that there is enough space to shift the kernel.\n",
        "Next, we implement this process in the `corr2d` function,\n",
        "which accepts an input tensor `X` and a kernel tensor `K`\n",
        "and returns an output tensor `Y`.\n"
      ],
      "metadata": {
        "id": "atD7jf1N5ePF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 1,
        "tab": [
          "mxnet"
        ],
        "id": "7R7baCJb3_QM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "mxnet"
        ],
        "id": "IzbwCLp53_QN"
      },
      "outputs": [],
      "source": [
        "def corr2d(X, K):\n",
        "    # compute 2D cross-correlation\n",
        "    h, w = K.shape\n",
        "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i,j].assign(tf.reduce_sum(\n",
        "                X[i: (i+h), j: (j+w)] * K\n",
        "            ))\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 5,
        "id": "aPFoNKgB3_QN"
      },
      "source": [
        "We can construct the input tensor `X` and the kernel tensor `K`\n",
        "from the figure above\n",
        "to **validate the output of the above implementation**\n",
        "of the two-dimensional cross-correlation operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 6,
        "tab": [
          "mxnet"
        ],
        "id": "kxgLLuQY3_QN"
      },
      "outputs": [],
      "source": [
        "X = tf.constant([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "Y = tf.constant([[0.0, 1.0], [2.0, 3.0]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr2d(X, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxSG5I4tuw7L",
        "outputId": "9e03060b-2366-4d06-f1ac-978990772a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
              "array([[19., 25.],\n",
              "       [37., 43.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 7,
        "id": "pYR8fpA13_QO"
      },
      "source": [
        "## Convolutional Layers\n",
        "\n",
        "A convolutional layer cross-correlates the input and kernel\n",
        "and adds a scalar bias to produce an output.\n",
        "The two parameters of a convolutional layer\n",
        "are the kernel and the scalar bias.\n",
        "When training models based on convolutional layers,\n",
        "we typically initialize the kernels randomly,\n",
        "just as we would with a fully-connected layer.\n",
        "\n",
        "We are now ready to **implement a two-dimensional convolutional layer**\n",
        "based on the `corr2d` function defined above.\n",
        "In the `build()` method,\n",
        "we set `weight` and `bias` as the two model parameters.\n",
        "The `call()` method\n",
        "calls the `corr2d` function and adds the bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 8,
        "tab": [
          "mxnet"
        ],
        "id": "rSgUOUPO3_QP"
      },
      "outputs": [],
      "source": [
        "class Conv2D(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def build(self, kernal_size):\n",
        "    initializer = tf.random_normal_initializer()\n",
        "    self.weight = self.add_weights(name='w', shape = kernal_size, initializer = initializer)\n",
        "    self.bias = self.add_weight(name='b', shape=(1, ), intializer = initializer)\n",
        "\n",
        "  def call(self):\n",
        "    return corr2d(inputs, self.weght) + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "UsRjcAKl3_QP"
      },
      "source": [
        "In\n",
        "$h \\times w$ convolution\n",
        "or a $h \\times w$ convolution kernel,\n",
        "the height and width of the convolution kernel are $h$ and $w$, respectively.\n",
        "We also refer to\n",
        "a convolutional layer with a $h \\times w$\n",
        "convolution kernel simply as a $h \\times w$ convolutional layer.\n",
        "\n",
        "\n",
        "## Object Edge Detection in Images\n",
        "\n",
        "Let us take a moment to parse [**a simple application of a convolutional layer:\n",
        "detecting the edge of an object in an image**]\n",
        "by finding the location of the pixel change.\n",
        "First, we construct an \"image\" of $6\\times 8$ pixels.\n",
        "The middle four columns are black (0) and the rest are white (1).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvCqbemWwq6T",
        "outputId": "711a9c29-872e-47a5-ab7e-fe7588a12a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 12,
        "tab": [
          "mxnet"
        ],
        "id": "JKl4exag3_QP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8072a731-74d4-4e1f-8f6e-7c7b32874d2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 0., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X = tf.Variable(tf.ones((6, 8)))\n",
        "X[:, 2:6].assign(tf.zeros(X[:,2:6].shape))\n",
        "X.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxSoZmG0xIvh",
        "outputId": "b7c4eea4-36f6-44cf-d5ed-3fbc5d7579ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([6, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(tf.reshape(tf.transpose(X), (8, 6)), cmap = 'gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yr_fgR9uyNpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1b14e09b-df43-4b2a-92b9-133547f2eecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "QOqtdGs33_QQ"
      },
      "source": [
        "Next, we construct a kernel `K` with a height of 1 and a width of 2.\n",
        "When we perform the cross-correlation operation with the input,\n",
        "if the horizontally adjacent elements are the same,\n",
        "the output is 0. Otherwise, the output is non-zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 15,
        "tab": [
          "mxnet"
        ],
        "id": "f1zaUw3z3_QQ"
      },
      "outputs": [],
      "source": [
        "K = tf.constant([[1.0, -1.0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "tplu2M573_QQ"
      },
      "source": [
        "We are ready to perform the cross-correlation operation\n",
        "with arguments `X` (our input) and `K` (our kernel).\n",
        "As you can see, **we detect 1 for the edge from white to black\n",
        "and -1 for the edge from black to white.**\n",
        "All other outputs take value 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "mxnet"
        ],
        "id": "L1nlE6uR3_QR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f492f277-d293-484d-ce97-5d0bd8669414"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
              "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "Y = corr2d(X, K)\n",
        "Y.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv2dfKpUyEHf",
        "outputId": "2664acd5-813f-412e-cb63-b3417c750a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([6, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(tf.reshape(tf.transpose(Y), (7, 6)), cmap = 'gray')\n",
        "plt.show()\n",
        "# larger when we have edge "
      ],
      "metadata": {
        "id": "xIzZu0l9zGQF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "80e7ec0d-05e1-45cf-a2a1-168b6dbedabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 18,
        "id": "PgfwJEF43_QR"
      },
      "source": [
        "We can now apply the kernel to the transposed image.\n",
        "As expected, it vanishes. **The kernel `K` only detects vertical edges.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "mxnet"
        ],
        "id": "020DT5te3_QR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUuvqWFtzuFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "H4zVPP-S3_QR"
      },
      "source": [
        "## Learning a Kernel\n",
        "\n",
        "Designing an edge detector by finite differences `[1, -1]` is neat\n",
        "if we know this is precisely what we are looking for.\n",
        "However, as we look at larger kernels,\n",
        "and consider successive layers of convolutions,\n",
        "it might be impossible to specify\n",
        "precisely what each filter should be doing manually.\n",
        "\n",
        "Now let us see whether we can **learn the kernel that generated `Y` from `X`**\n",
        "by looking at the input--output pairs only.\n",
        "We first construct a convolutional layer\n",
        "and initialize its kernel as a random tensor.\n",
        "Next, in each iteration, we will use the squared error\n",
        "to compare `Y` with the output of the convolutional layer.\n",
        "We can then calculate the gradient to update the kernel.\n",
        "For the sake of simplicity,\n",
        "in the following\n",
        "we use the built-in class\n",
        "for two-dimensional convolutional layers\n",
        "and ignore the bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 21,
        "tab": [
          "mxnet"
        ],
        "id": "-u_p84WV3_QR"
      },
      "outputs": [],
      "source": [
        "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
        "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
        "\n",
        "conv2d = tf.keras.layers.Conv2D(1, (1,2), use_bias = False)\n",
        "# The two-dimensional convolutional layer uses four-dimensional input and\n",
        "# output in the format of (example, height, width, channel), where the batch\n",
        "# size (number of examples in the batch) and the number of channels are both 1\n",
        "\n",
        "X = tf.reshape(X, (1, 6, 8, 1))\n",
        "Y = tf.reshape(Y, (1, 6, 7, 1))\n",
        "\n",
        "lr = 2e-2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_hat = conv2d(X)\n",
        "for i in range(10):\n",
        "  with tf.GradientTape() as tape:\n",
        "    Y_hat = conv2d(X)\n",
        "    loss = tf.reduce_sum(tf.abs(Y_hat - Y)**2)\n",
        "    loss_gradient = tape.gradient(loss, conv2d.weights[0])\n",
        "    update = tf.multiply(lr, loss_gradient)\n",
        "    weights = conv2d.get_weights()\n",
        "    weights[0] = conv2d.weights[0] - update\n",
        "    # apply calculated weights to original layer\n",
        "    conv2d.set_weights(weights) \n",
        "    if (i+1) % 2 == 0:\n",
        "      print(f'epoch {i+1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3NOHEhT4ly_",
        "outputId": "ab183f92-5749-4f67-c25d-134b2a2890d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2\n",
            "epoch 4\n",
            "epoch 6\n",
            "epoch 8\n",
            "epoch 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 24,
        "id": "4DsH2KWz3_QS"
      },
      "source": [
        "Note that the error has dropped to a small value after 10 iterations. Now we will **take a look at the kernel tensor we learned.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "mxnet"
        ],
        "id": "AJavowTM3_QS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a0f947-79cc-4d5e-f622-f4e7fa68ce9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.89287496, -0.89287496]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tf.reshape(conv2d.get_weights()[0], (1, 2)).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 28,
        "id": "MHVae6SF3_QS"
      },
      "source": [
        "Indeed, the learned kernel tensor is remarkably close\n",
        "to the kernel tensor `K` we defined earlier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Cross-Correlation and Convolution\n",
        "\n",
        "Recall our earlier observation of the correspondence\n",
        "between the cross-correlation and convolution operations.\n",
        "Here let us continue to consider two-dimensional convolutional layers.\n",
        "What if such layers\n",
        "perform strict convolution operations\n",
        "instead of cross-correlations?\n",
        "In order to obtain the output of the strict *convolution* operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the *cross-correlation* operation with the input tensor.\n",
        "\n",
        "It is noteworthy that since kernels are learned from data in deep learning,\n",
        "the outputs of convolutional layers remain unaffected\n",
        "no matter such layers\n",
        "perform\n",
        "either the strict convolution operations\n",
        "or the cross-correlation operations.\n",
        "\n",
        "To illustrate this, suppose that a convolutional layer performs *cross-correlation* and learns the kernel in the first figure above, which is denoted as the matrix $\\mathbf{K}$ here.\n",
        "Assuming that other conditions remain unchanged,\n",
        "when this layer performs strict *convolution* instead,\n",
        "the learned kernel $\\mathbf{K}'$ will be the same as $\\mathbf{K}$\n",
        "after $\\mathbf{K}'$ is\n",
        "flipped both horizontally and vertically.\n",
        "That is to say,\n",
        "when the convolutional layer\n",
        "performs strict *convolution*\n",
        "for the input in the figure\n",
        "and $\\mathbf{K}'$,\n",
        "the same output in the figure\n",
        "(cross-correlation of the input and $\\mathbf{K}$)\n",
        "will be obtained.\n",
        "\n",
        "In keeping with standard terminology with deep learning literature,\n",
        "we will continue to refer to the cross-correlation operation\n",
        "as a convolution even though, strictly-speaking, it is slightly different.\n",
        "Besides,\n",
        "we use the term *element* to refer to\n",
        "an entry (or component) of any tensor representing a layer representation or a convolution kernel.\n",
        "\n"
      ],
      "metadata": {
        "id": "ObvRga0X8_66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Feature Map and Receptive Field\n",
        "\n",
        "![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+1\\times1+3\\times2+4\\times3=19$.](http://d2l.ai/_images/correlation.svg)\n",
        "\n",
        "The convolutional layer output in\n",
        "the figure above\n",
        "is sometimes called a *feature map*,\n",
        "as it can be regarded as\n",
        "the learned representations (features)\n",
        "in the spatial dimensions (e.g., width and height)\n",
        "to the subsequent layer.\n",
        "In CNNs,\n",
        "for any element $x$ of some layer,\n",
        "its *receptive field* refers to\n",
        "all the elements (from all the previous layers)\n",
        "that may affect the calculation of $x$\n",
        "during the forward propagation.\n",
        "Note that the receptive field\n",
        "may be larger than the actual size of the input.\n",
        "\n",
        "Let us continue to use the figure above to explain the receptive field.\n",
        "Given the $2 \\times 2$ convolution kernel,\n",
        "the receptive field of the shaded output element (of value $19$)\n",
        "is\n",
        "the four elements in the shaded portion of the input.\n",
        "Now let us denote the $2 \\times 2$\n",
        "output as $\\mathbf{Y}$\n",
        "and consider a deeper CNN\n",
        "with an additional $2 \\times 2$ convolutional layer that takes $\\mathbf{Y}$\n",
        "as its input, outputting\n",
        "a single element $z$.\n",
        "In this case,\n",
        "the receptive field of $z$\n",
        "on $\\mathbf{Y}$ includes all the four elements of $\\mathbf{Y}$,\n",
        "while\n",
        "the receptive field\n",
        "on the input includes all the nine input elements.\n",
        "Thus,\n",
        "when any element in a feature map\n",
        "needs a larger receptive field\n",
        "to detect input features over a broader area,\n",
        "we can build a deeper network.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xZHVj5Tk89Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "* The core computation of a two-dimensional convolutional layer is a two-dimensional cross-correlation operation. In its simplest form, this performs a cross-correlation operation on the two-dimensional input data and the kernel, and then adds a bias.\n",
        "* We can design a kernel to detect edges in images.\n",
        "* We can learn the kernel's parameters from data.\n",
        "* With kernels learned from data, the outputs of convolutional layers remain unaffected regardless of such layers' performed operations (either strict convolution or cross-correlation).\n",
        "* When any element in a feature map needs a larger receptive field to detect broader features on the input, a deeper network can be considered.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4LtRuV9z85Sx"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}